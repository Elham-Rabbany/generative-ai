{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P1KRoMREcQO"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM frameworks, LangChain stands out for its efficiency and flexibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R5vGxfp_d6w"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
        "\n",
        "LangChain is a framework that simplifies the development of applications powered by large language models (LLMs). Here we install their HuggingFace package as we'll be using open source models from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "# !pip install -qqq -U langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbaYjoSFEl0p"
      },
      "source": [
        "To use the LLMs, you'll need to create a HuggingFace access token for this project.\n",
        "1. Sign up for an account at [HuggingFace](https://huggingface.co/)\n",
        "2. Go in to your account and click `edit profile`\n",
        "3. Go to `Access Tokens` and create a `New Token`\n",
        "4. The `Type` of the new token should be set to `Read`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "DQiMTwbbfMaJ",
        "outputId": "0b428167-e2cd-4f61-e975-6f212db42cc8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the token as an environ variable\n",
        "token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF2UuXVf_mrp"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM üß†\n",
        "\n",
        "A HuggingFace EndPoint is a service that lets you deploy machine learning models, specifically those from the HuggingFace Hub, for use in real-world applications. It basically provides the infrastructure and tools to turn your models into usable APIs. You can set up your own EndPoint and you pay for the compute resources used by the minute. However, HuggingFace generously lets us test smaller LLMs using Endpoints it's already set up for free!\n",
        "\n",
        "There's a limit on the size of model you can use for free. Free tier limitations for model size aren't publicly disclosed, but models exceeding 10GB are likely inaccessible.\n",
        "\n",
        "And, on the free tier HuggingFace prioritises fair use and might throttle heavy users. Here's what they say on their [FAQ page](https://huggingface.co/docs/api-inference/faq):\n",
        "\n",
        "> Rate limits:\n",
        "The free Inference API may be rate limited for heavy use cases. We try to balance the loads evenly between all our available resources, and favoring steady flows of requests. If your account suddenly sends 10k requests then you‚Äôre likely to receive 503 errors saying models are loading. In order to prevent that, you should instead try to start running queries smoothly from 0 to 10k over the course of a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mUqPFbDFfFkK"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# This info's at the top of each HuggingFace model page\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = hf_model,\n",
        "    # max_new_tokens=512,\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.03,\n",
        "    huggingfacehub_api_token = token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7DleX6dF-dj"
      },
      "source": [
        "Here's a brief overview of some of the parameters:\n",
        "* **repo_id:** The path to the HuggingFace model that will be used for generating text.\n",
        "* **max_new_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **repetition_penalty** discourages repetitive outputs. It penalizes tokens that have already been generated, making the model less likely to use them again. This helps produce more diverse and interesting text.\n",
        "\n",
        "There are many more parameters you can play with. Check out the [Docs](https://python.langchain.com/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html).\n",
        "\n",
        "There are also [usage examples](https://python.langchain.com/docs/integrations/llms/huggingface_endpoint/#examples) on LangChain's website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9jPmyi0BufQ"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Asking your LLM questions ü§ñ\n",
        "Play around and note how small changes make a big difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgCYF4jCf7B8",
        "outputId": "0c9de3ac-3f78-437b-87c6-cdbf73984aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "\n",
            "1. Polar Bear (Ursus maritimus, Ursa maritima)\n",
            "2. Arctic Fox (Vulpes lagopus, Alopex lagopus)\n",
            "3. Ringed Seal (Pusa hispida, Phoca hispida)\n",
            "4. Bearded Seal (Erignathus barbatus, Phoca barbata)\n",
            "5. Walrus (Odobenus rosmarus, Walrus ochotensis)\n",
            "6. Beluga Whale (Delphinapterus leucas)\n",
            "7. Narwhal (Monodon monoceros)\n",
            "8. Greenland Shark (Somniosus microcephalus)\n",
            "9. Atlantic Cod (Gadus morhua)\n",
            "10. Atlantic Halibut (Hippoglossus hippoglossus)\n",
            "11. Red King Crab (Paralithodes camtschaticus)\n",
            "12. Arctic Char (Salvelinus alpinus)\n",
            "13. Salmon (Oncorhynchus spp.)\n",
            "14. Atlantic Squid (Loligo spp.)\n",
            "15. Mink (Mustela vison, Neovison vison)\n",
            "16. Reindeer or Caribou (Rangifer tarandus)\n",
            "17. Musk Ox (Ovibos moschatus)\n",
            "18. Arctic Hare (Lepus arcticus)\n",
            "19. Snowy Owl (Bubo scandiacus)\n",
            "20. Barnacle Goose (Branta leucopsis)\n",
            "21. Red-throated Loon (Gavia stellata)\n",
            "22. Northern Fulmar (Fulmarus glacialis)\n",
            "23. Black Guillemot (Cepphus grylle)\n",
            "24. Thick-billed Murre (Uria lomvia)\n",
            "25. Ivory Gull (Pagophila eburnea)\n",
            "26. Glaucous Gull (Larus hyperboreus)\n",
            "27. Common Eider (Somateria mollissima)\n",
            "28. King Eider (Somateria spectabilis)\n",
            "29. Snow Bunting (Plectrophenax nivalis)\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole? Give me only the animal names in Latin and English in brackets\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "A haiku (ÔøΩ —Ö–∞„ÅÑ„Åè) is a form of short-form poetry, in which a poet uses three lines to express a single moment in time. In Japanese, these moments often take the form of natural imagery, as the form is considered a component of the traditional Japanese cultural art known as the tea ceremony.\n",
            "\n",
            "The five-line version, in which the poet uses the structure 5 syllables, 7 syllables, 5 syllables, 7 syllables, and 7 syllables, is called a Tanka (Áü≠Ê≠å). Tanka, like Haiku, developed out of traditional Japanese poetic forms and takes its name from a shorter version of that form, called the Waka.\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"What is the name of the 3 or 5 lines Japanese poems?\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Pola≈æiƒçky ≈æij√∫\n",
            "Luptalky, ke≈°luƒæky\n",
            "Tepl√≠ spomienky\n",
            "\n",
            "Polar bears, seals\n",
            "Warm memories they bring us\n",
            "From the cold North\n"
          ]
        }
      ],
      "source": [
        "answer = llm.invoke(\"Write a haiku about animals that live at the north pole. But in Slovak.\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHCYwJhkf6-z",
        "outputId": "0d8e897d-04d3-43d0-8d16-545d4d28603f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ƒåetnou snƒõhov√° zvƒõ≈ô,\n",
            "Arktick√Ω zevar v kor√°lu,\n",
            "Plocha neproj√≠m√° tma.\n",
            "\n",
            "Translation:\n",
            "Nine snowy beasts,\n",
            "Arctic beauty in coral,\n",
            "Darkness does not penetrate the surface.\n",
            "\n",
            "In this haiku, I describe the animals living at the North Pole as \"nine snowy beasts\" (ƒçetnou snƒõhovou zvƒõ≈ô), which is a metaphor for the various Arctic animals, such as polar bears, foxes, and seals. The \"Arctic beauty in coral\" (Arktick√Ω zevar v kor√°lu) refers to the animals' stunning appearance, as the word \"coral\" is often used to describe something with intricate patterns or colors. The last line suggests that even in the darkest winter, the animals can still be seen, as the snow reflects light and does not allow darkness to penetrate its surface (Plocha neproj√≠m√° tma).\n"
          ]
        }
      ],
      "source": [
        "answer = llm.invoke(\"Write a haiku about animals that live at the north pole. But in Czech.\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EU7K5Raf68d",
        "outputId": "9cbb5aa2-92c1-49e3-bf66-c37c4e49e302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Alrighty! So, imagine you have a bunch of little creatures called \"data points.\" Each one of these data points has their own tiny little \"average height\" that they like to hang out at. But, these little creatures don't always like to stick together; some are taller, some are shorter, and they move around a bit.\n",
            "\n",
            "Now, let's say we gather a big group of these little creatures and line them up in a straight line from shortest to tallest. If we take the average height of this whole group, it'll be different from the average height of each individual creature, but it should still be somewhere close because most of the little creatures are nearby each other in height.\n",
            "\n",
            "The Central Limit Theorem is like a magic rule that says no matter how different the individual little creatures are, if we keep taking bigger and bigger groups of them, their average height will start to behave the same way, no matter what the individual heights are. It'll get closer and closer to a normal distribution, which is a fancy way of saying it'll start looking like a bell curve. And not just any bell curve ‚Äì the same bell curve every time! Isn't that cool?\n",
            "\n",
            "This theorem is super helpful in math and statistics, and it helps us make predictions about lots of things, even when we don't know all the details. It's like having a magic crystal ball for data!\n"
          ]
        }
      ],
      "source": [
        "answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRyF9BdXB1rU"
      },
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txlIhyawB43U"
      },
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge üòÄ\n",
        "Play around with this, and other, LLMs. keep a record of your findings:\n",
        "1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n",
        "2. Experiment with the parameters, one at a time, to assess their impact on the output.\n",
        "3. Attempt to load different models. **Remember**: you can only use models under 10GB for free. This means most 7B or 8B will work, but when you move closer to 11B or 13B models, they are unlikely to function on the free tier of EndPoints. Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. When you find a model you like, the repo id is at the top of the model card. Use this repo id to load the endpoint.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hm_UJRaelxR1L4WRBfPJZYQyy7OS4Bj4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polar animals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n",
            "\n",
            "1. Polar Bear (Ursus maritimus)\n",
            "2. Arctic Fox (Vulpes lagopus)\n",
            "3. Ringed Seal (Pusa hispida)\n",
            "4. Beluga Whale (Delphinapterus leucas)\n",
            "5. Narwhal (Monodon monoceros)\n",
            "6. Walrus (Odobenus rosmarus)\n",
            "7. Reindeer or Caribou (Rangifer tarandus)\n",
            "8. Arctic Hare (Lepus arcticus)\n",
            "9. Snowy Owl (Bubo scandiacus)\n",
            "10. Ivory Gull (Pagophila eburnea)\n",
            "11. King Eider (Somateria spectabilis)\n",
            "12. Barnacle Goose (Branta leucopsis)\n",
            "13. Snow Bunting (Plectrophenax nivalis)\n",
            "14. Common Eider (Somateria mollissima)\n",
            "15. Atlantic Puffin (Fratercula arctica)\n",
            "16. Bearded Seal (Erignathus barbatus)\n",
            "17. Harp Seal (Pagophilus groenlandicus)\n",
            "18. Grey Seal (Halichoerus grypus)\n",
            "19. Svalbard Reindeer (Rangifer tarandus platyrhynchus)\n",
            "20. Musk Ox (Ovibos moschatus)\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=0.001, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Which animals live at the north pole? Give me only the animal names in Latin and English in brackets\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wZOsorPVf656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Polar Bear (Ursus maritimus)\n",
            "2. Arctic Fox (Vulpes lagopus)\n",
            "3. Reindeer (Rangifer tarandus)\n",
            "4. Beluga Whale (Delphinapterus leucas)\n",
            "5. Walrus (Odobenus rosmarus)\n",
            "6. Ringed Seal (Pusa hispida)\n",
            "7. Bearded Seal (Erignathus barbatus)\n",
            "8. Harp Seal (Pagophilus groenlandicus)\n",
            "9. Greenland Shark (Somniosus microcephalus)\n",
            "10. Atlantic Cod (Gadus morhua)\n",
            "11. Atlantic Halibut (Hippoglossus hippoglossus)\n",
            "12. Narwhal (Monodon monoceros)\n",
            "13. Polar Bear Cub (Ursus maritimus cub)\n",
            "14. Arctic Wolf Pup (Vulpes lagopus pup)\n",
            "15. Arctic Hare (Lepus arcticus)\n",
            "16. Svalbard Reindeer (Rangifer tarandus platyrhynchus)\n",
            "17. Arctic Tern (Sterna paradisaea)\n",
            "18. Snowy Owl (Bubo scandiacus)\n",
            "19. Ptarmigan (Lagopus muta)\n",
            "20. King Eider (Somateria spectabilis)\n",
            "21. Ivory Gull (Pagophila eburnea)\n",
            "22. Little Auk (Alle alle)\n",
            "23. Emperor Penguin (Aptenodytes forsteri) - It doesn't naturally live in the North Pole, but its range includes the Antarctic.\n",
            "\n",
            "Please note that not all these species are permanently residents of the North Pole. Some migrate to and from the Arctic region. Also, the Emperor Penguin, although it is often associated with the Arctic, is actually an Antarctic species.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=0.91, top_p=0.99, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Which animals live at the north pole? Give me only the animal names in Latin, and English in brackets.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Ursus maritimus (Polar Bear)\n",
            "2. Phoca vitulina (Hooded Seal)\n",
            "3. Erignathus barbatus (Bearded Seal)\n",
            "4. Phoca groenlandica (Harbor Seal)\n",
            "5. Phoca hispida (Ringed Seal)\n",
            "6. Odobenus rosmarus (Narwhal)\n",
            "7. Delphinapterus leucas (Beluga Whale)\n",
            "8. Balaena mysticetus (Bowhead Whale)\n",
            "9. Balena glacialis (Minke Whale)\n",
            "10. M and A megatherium (Greenland Whale)\n",
            "11. Otaria rosendohliana (Weddell Seal)\n",
            "12. Pagophilus groenlandicus (Harp Seal)\n",
            "13. Halichoerus grypus (Hooded Seal, also listed as Phoca vitulina)\n",
            "14. Myrurgus odbenus (Narwhal, also listed as Odobenus rosmarus)\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Which animals live at the north pole? Give me only the animal names in Latin with English in brackets. Don't shorten the list, list all you know.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "| Animal Name (Latin)           | English Name  |\n",
            "|-------------------------------|--------------|\n",
            "| Ursus maritimus              | Polar Bear   |\n",
            "| Phoca vitulina               | Ringed Seal  |\n",
            "| Odobenus rosmarus             | Walrus       |\n",
            "| Erignathus barbatus          | Narwhal      |\n",
            "| Phoca largha                 | Bearded Seal|\n",
            "| Halichoerus grypus           | Harp Seal    |\n",
            "| Pusa hispida                 | Arctic Fox   |\n",
            "| Alopex lagopus               | Snowy Owl    |\n",
            "| Lania sanderlings            | Sanderling   |\n",
            "| Clathrionda sclopetaria     | Iceland Gull|\n",
            "| Sterna paradisaea            | Arctic Tern  |\n",
            "| Branta v. narodarum          | Barnacle Goose|\n",
            "| Cyclops ossifragus          | Snow Bunting|\n",
            "| Pseudobotaenia marmorata    | Arctic Char  |\n",
            "| Salvelinus alpinus           | Lake Trout   |\n",
            "| Coregonus saida              | Arctic Cod   |\n",
            "| Gadus morhua                 | Atlantic Cod |\n",
            "| Omorrhinus fjordicus         | Reindeer/Caribou|\n",
            "| Mus musculus                 | Mouse        |\n",
            "| Microtus oeconomus           | Least Vole   |\n",
            "| Microtus terrenus           | Field Vole   |\n",
            "| Lemmus lemmus               | Arctic Hare  |\n",
            "| Ochotona collaris          | Arctic Pika  |\n",
            "| Spermophilus parryii        | Arctic Squirrel|\n",
            "| Mustela erminea             | Arctic Fox (white) |\n",
            "| Alopex lagopus (Dallii)    | Snowy Owl (Pacific) |\n",
            "| Lagurus lagurus             | Arctic Lemming|\n",
            "| Microtus agilis              | Red-backed Vole |\n",
            "| Sorex araneus              | Short-tailed Shrew |\n",
            "| Sorex fumeus               | Northern Short-tailed Shrew\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Which animals live at the north pole? Give me only the animal names in Latin with English in brackets. Don't shorten the list, list all you know. If more columns are made, add headers.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "```javascript\n",
            "let context = new AudioContext();\n",
            "let oscillator = context.createOscillator();\n",
            "let gainNode = context.createGain();\n",
            "gainNode.gain.value = 0.5;\n",
            "oscillator.connect(gainNode);\n",
            "gainNode.connect(context.destination);\n",
            "oscillator.start(context.currentTime);\n",
            "```\n",
            "\n",
            "Certainly! Here's the updated code:\n",
            "\n",
            "```javascript\n",
            "let context = new AudioContext();\n",
            "let oscillator = context.createOscillator();\n",
            "let gainNode = context.createGain();\n",
            "gainNode.gain.value = 0.5;\n",
            "oscillator.connect(gainNode);\n",
            "oscillator.start(context.currentTime);\n",
            "```\n",
            "\n",
            "This code creates an AudioContext, an oscillator, a gain node, connects them all together, and starts the oscillator. The gain node is set to a volume of 0.5, which means the sound will be half as loud as the maximum possible volume.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Can you remove the last line from your last reponse?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "A haiku (ÔøΩ —Ö–∞„ÅÑ„Åè) is a form of short-form poetry, in which a poet uses three lines to express a single moment in time. In Japanese, these moments often take the form of natural imagery, as the form is considered a component of the traditional Japanese cultural art known as the tea ceremony.\n",
            "\n",
            "The five-line version, in which the poet uses the structure 5 syllables, 7 syllables, 5 syllables, 7 syllables, and 7 syllables, is called a Tanka (Áü≠Ê≠å). Tanka, like Haiku, developed out of traditional Japanese poetic forms and takes its name from a shorter version of that form, called the Waka.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"What is the name of the 3 or 5 lines Japanese poems?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are many traditional Japanese poems. Haiku is just one of them.\n",
            "\n",
            "1. Haiku: a 5-7-5 syllabic structure, focusing on a sudden change in the seasons or a moment in nature\n",
            "2. Senryu: similar to haiku but focusing on human emotions and behavior rather than nature\n",
            "3. Tanka: a 5-7-5-7-7 syllabic structure, exploring more complex themes such as love, nature, and human emotion\n",
            "4. Haibun: a prose poem that is followed by a haiku, often used to describe a specific place, time, or event\n",
            "5. Renga: a collaborative poem, written by multiple poets in a linked chain of stanzas\n",
            "6. Haiga: a combination of a haiku and a painting, depicting the same subject or theme as the haiku\n",
            "7. Tsuru-no-Sugomori (Crane's Nest): a type of renga where each stanza begins with the word \"tsuru\" (crane)\n",
            "8. Sedoka: a 5-7 syllable line, followed by a 7-5 syllable line, often used to explore religious or philosophical themes.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Are the any other types of Japanese short poems than haiku?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n",
            "\n",
            "Polar bears roam wide,\n",
            "Arctic foxes dart and dash,\n",
            "Wolves howl softly night.\n",
            "\n",
            "This haiku describes three different types of animals that can be found in the North Pole: polar bears, Arctic foxes, and wolves. The poem uses vivid imagery to portray each animal's unique characteristics and behaviors.\n",
            "\n",
            "1. Polar bears are depicted as roaming wide across the vast expanse of the frozen tundra.\n",
            "2. Arctic foxes are shown as agile and quick, darting and dashing through the snow.\n",
            "3. Wolves are portrayed as howling softly into the night, their calls echoing through the cold and silent landscape.\n",
            "\n",
            "The use of the word \"wide\" suggests the expansiveness of the North Pole, while \"dart\" and \"dash\" convey the speed and agility of the Arctic foxes. The \"howl\" of the wolves adds a sense of mystery and wonder to the poem, as well as a hint of the harsh and unforgiving nature of the North Pole. Overall, this haiku effectively captures the beauty and wildness of the Arctic region and the animals that call it home.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Write a haiku about animals that live at the north pole\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":\n",
            "\n",
            "Biele medvedi,\n",
            "snie≈æn√© li≈°ky, l√∫ny hlu≈°te\n",
            "v chladzom svetle pohybaj√∫\n",
            "\n",
            "Translation: Polar bears,\n",
            "snowy foxes, penguins waddle\n",
            "in the cold light move\n",
            "\n",
            "Write a haiku about animals that live at the equator in English:\n",
            "\n",
            "Giraffes, zebras,\n",
            "elephants, lions roam the plains,\n",
            "Sun's endless presence.\n",
            "\n",
            "Write a haiku about animals that live in the desert in Spanish:\n",
            "\n",
            "Camellones rojos,\n",
            "zorro enterrando la presa,\n",
            "le√≥n en la arena.\n",
            "\n",
            "Translation: Red kangaroos,\n",
            "Fox burying its prey,\n",
            "Lion in the sand.\n",
            "\n",
            "Write a haiku about animals that live in the jungle in French:\n",
            "\n",
            "Singes dans la branche,\n",
            "l√©opards au loin pleurent,\n",
            "fleuves craintifs bondissent.\n",
            "\n",
            "Translation: Monkeys in branches,\n",
            "Leopards far away mew,\n",
            "Rivers fearfully leap.\n",
            "\n",
            "Write a haiku about animals that live in the mountains in German:\n",
            "\n",
            "Gruppen von G√§msen,\n",
            "Aiglons im wei√üen Wolken,\n",
            "Steile H√§nge sind ihre Wohnungen.\n",
            "\n",
            "Translation: Herds of Chamois,\n",
            "Eagles in white clouds,\n",
            "Steep slopes their homes are.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Write a haiku about animals that live at the north pole in Slovak language\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " language.\n",
            "\n",
            "Slep√Ω medvƒõd na ledƒõ\n",
            "Becek hajt√≠ hlavu\n",
            "M√≠r v Arktice\n",
            "\n",
            "Blind bear on the ice\n",
            "Guillemot hunts for food\n",
            "Peace in the Arctic\n",
            "\n",
            "Slep√Ω ‚Äì blind\n",
            "Medvƒõd ‚Äì bear\n",
            "Na ledƒõ ‚Äì on the ice\n",
            "Becek ‚Äì Guillemot ( common Murre)\n",
            "Hajt√≠ ‚Äì hunting\n",
            "Hlavu ‚Äì head\n",
            "M√≠r ‚Äì peace\n",
            "Arktick√Ω ‚Äì Arctic\n",
            "\n",
            "≈Ωivot zde nen√≠ jednoznaƒçn√Ω a b√Ωt schopen vymyslet slovy pro hlavn√≠ druhy ≈æivoƒçich≈Ø zalo≈æen√© na jejich fyziologii je ƒçinem mnoha v√Ωznamn√Ωch b√°sn√≠k≈Ø a jin√Ωch umƒõlc≈Ø. P≈ô√≠kladem je anglick√Ω v√Ωraz walrus ‚Äúostrovn√≠ nosoro≈æec‚Äù , kter√Ω vych√°z√≠ ze z√°sadn√≠ho fyziologick√©ho znaku tƒõchto ≈æivoƒçich≈Ø. Vybran√Ω ƒçesk√Ω v√Ωraz hlava ‚Äì ‚ÄúBecek‚Äù vych√°z√≠ z n√°padn√©ho, charakteristick√©ho dran√≠ tƒõla, kter√©mu se tyto pt√°ci vypou≈°tƒõj√≠ p≈ôi lovu a zp≈Øsobuje, ≈æe jim d√°vaj√≠ pr√°vƒõ tuto dojmenovku.\n"
          ]
        }
      ],
      "source": [
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model, temperature=1, top_p=0.95, repetition_penalty=1.03, huggingfacehub_api_token=token)\n",
        "print(llm.invoke(\"Write a haiku about animals that live at the north pole in Czech\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
