{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative_ai/blob/main/1_large_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM frameworks, LangChain stands out for its efficiency and flexibility."
      ],
      "metadata": {
        "id": "6P1KRoMREcQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
        "\n",
        "LangChain is a framework that simplifies the development of applications powered by large language models (LLMs). Here we install their HuggingFace package as we'll be using open source models from HuggingFace."
      ],
      "metadata": {
        "id": "2R5vGxfp_d6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the LLMs, you'll need to create a HuggingFace access token for this project.\n",
        "1. Sign up for an account at [HuggingFace](https://huggingface.co/)\n",
        "2. Go in to your account and click `edit profile`\n",
        "3. Go to `Access Tokens` and create a `New Token`\n",
        "4. The `Type` of the new token should be set to `Read`\n",
        "\n",
        "We've then saved ours as a Colab secret - this way we can use it in multiple notebooks without having to type it or reveal it."
      ],
      "metadata": {
        "id": "VbaYjoSFEl0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the token as an environ variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "DQiMTwbbfMaJ",
        "outputId": "0b428167-e2cd-4f61-e975-6f212db42cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HF_TOKEN does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c3ced596627f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set the token as an environ variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUGGINGFACEHUB_API_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_TOKEN does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM üß†\n",
        "\n",
        "A HuggingFace EndPoint is a service that lets you deploy machine learning models, specifically those from the HuggingFace Hub, for use in real-world applications. It basically provides the infrastructure and tools to turn your models into usable APIs. You can set up your own EndPoint and you pay for the compute resources used by the minute. However, HuggingFace generously lets us test smaller LLMs using Endpoints it's already set up for free!\n",
        "\n",
        "There's a limit on the size of model you can use for free. Free tier limitations for model size aren't publicly disclosed, but models exceeding 10GB are likely inaccessible.\n",
        "\n",
        "And, on the free tier HuggingFace prioritises fair use and might throttle heavy users. Here's what they say on their [FAQ page](https://huggingface.co/docs/api-inference/faq):\n",
        "\n",
        "> Rate limits:\n",
        "The free Inference API may be rate limited for heavy use cases. We try to balance the loads evenly between all our available resources, and favoring steady flows of requests. If your account suddenly sends 10k requests then you‚Äôre likely to receive 503 errors saying models are loading. In order to prevent that, you should instead try to start running queries smoothly from 0 to 10k over the course of a few minutes."
      ],
      "metadata": {
        "id": "iF2UuXVf_mrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# This info's at the top of each HuggingFace model page\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = hf_model,\n",
        "    # max_new_tokens=512,\n",
        "    temperature=0.01,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.03,\n",
        "    # huggingfacehub_api_token = \"your_hf_token\" # Instead of passing your HuggingFace token to os, you could include it here\n",
        ")"
      ],
      "metadata": {
        "id": "mUqPFbDFfFkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a brief overview of some of the parameters:\n",
        "* **repo_id:** The path to the HuggingFace model that will be used for generating text.\n",
        "* **max_new_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **repetition_penalty** discourages repetitive outputs. It penalizes tokens that have already been generated, making the model less likely to use them again. This helps produce more diverse and interesting text.\n",
        "\n",
        "There are many more parameters you can play with. Check out the [Docs](https://python.langchain.com/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html).\n",
        "\n",
        "There are also [usage examples](https://python.langchain.com/docs/integrations/llms/huggingface_endpoint/#examples) on LangChain's website."
      ],
      "metadata": {
        "id": "v7DleX6dF-dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3.&nbsp; Asking your LLM questions ü§ñ\n",
        "Play around and note how small changes make a big difference."
      ],
      "metadata": {
        "id": "T9jPmyi0BufQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n",
        "print(answer_1)"
      ],
      "metadata": {
        "id": "kgCYF4jCf7B8",
        "outputId": "0c9de3ac-3f78-437b-87c6-cdbf73984aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The Arctic is home to a variety of animals that have adapted to the cold, harsh environment. Here are some of the most common animals found in the Arctic:\n",
            "\n",
            "1. Polar Bears: The polar bear is the largest land carnivore and is well-adapted to life in the Arctic. They have a thick layer of body fat and a waterproof coat of fur that helps them stay warm and buoyant in the icy waters.\n",
            "\n",
            "2. Arctic Fox: The Arctic fox is a small, agile predator that is well-adapted to the cold. They have thick fur that changes color with the seasons, and they are excellent hunters of small mammals and birds.\n",
            "\n",
            "3. Reindeer: Reindeer, also known as caribou in North America, are large, hoofed mammals that are well-adapted to the Arctic. They have a thick coat of fur and a strong, curved antler that helps them navigate through the snow.\n",
            "\n",
            "4. Beluga Whale: The beluga whale is a white, toothed whale that lives in the Arctic waters. They are well-adapted to the cold and can dive to depths of up to 1,000 feet.\n",
            "\n",
            "5. Walrus: The walrus is a large, marine mammal that is well-adapted to life in the Arctic. They have a thick layer of blubber that helps them stay warm, and they have tusks that they use for hunting and fighting.\n",
            "\n",
            "6. Seals: There are several species of seals that live in the Arctic, including the harp seal, hooded seal, and ringed seal. They are well-adapted to life in the water and are important prey for polar bears and other predators.\n",
            "\n",
            "7. Arctic Tern: The Arctic tern is a migratory bird that breeds in the Arctic and spends the winter in the Antarctic. They are known for their long migration, which can cover up to 25,000 miles each way.\n",
            "\n",
            "8. Snowy Owl: The snowy owl is a large, white owl that is well-adapted to life in the Arctic. They have excellent vision and are skilled hunters of small mammals and birds.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n",
        "print(answer_2)"
      ],
      "metadata": {
        "id": "cHCYwJhkf6-z",
        "outputId": "0d8e897d-04d3-43d0-8d16-545d4d28603f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "In the realm where the sun seldom shines,\n",
            "A world of ice, where the cold winds whine,\n",
            "Lives a cast of creatures, so rare and fine,\n",
            "Their stories, I'll weave, in this rhyme of mine.\n",
            "\n",
            "The polar bear, the king of the snow,\n",
            "With fur as white as the winter's glow,\n",
            "He roams on the ice, his mighty paw prints show,\n",
            "A sight to behold, in this icy abode.\n",
            "\n",
            "The Arctic fox, in red and white,\n",
            "Dances on the tundra, in the soft moonlight,\n",
            "His coat changes with the changing night,\n",
            "A survivor, through the harshest fight.\n",
            "\n",
            "The walrus, with his tusks so long,\n",
            "In the icy ocean, he sings his song,\n",
            "His belly flops, as he moves along,\n",
            "A spectacle, in this vast expanse.\n",
            "\n",
            "The reindeer, with antlers grand,\n",
            "In the Arctic night, they take a stand,\n",
            "Guiding Santa's sleigh, across the land,\n",
            "A symbol of hope, in this frozen band.\n",
            "\n",
            "The snowy owl, with eyes so bright,\n",
            "Hunts in the darkness, with all his might,\n",
            "His silent flight, a ghostly sight,\n",
            "In the Arctic night, he takes his flight.\n",
            "\n",
            "The beluga whale, in the icy sea,\n",
            "Sings a melody, as pure as can be,\n",
            "His echoing calls, a haunting spree,\n",
            "In the Arctic silence, he sets his free.\n",
            "\n",
            "These creatures, so unique and wild,\n",
            "In the North Pole, they make their vile,\n",
            "A testament to nature's guile,\n",
            "In the heart of the Arctic, they beguile.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n",
        "print(answer_3)"
      ],
      "metadata": {
        "id": "_EU7K5Raf68d",
        "outputId": "9cbb5aa2-92c1-49e3-bf66-c37c4e49e302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Alright, let's imagine you have a big bag of candies. Each candy has a different weight, but let's say the average weight of all candies is 10 grams and the weights are spread out around that average.\n",
            "\n",
            "Now, if you pick 5 candies at random from the bag, the total weight of those 5 candies might not be exactly 50 grams (10 grams per candy). It could be more or less. But if you do this many times, the average total weight of the candies you picked will usually be pretty close to 50 grams.\n",
            "\n",
            "The central limit theorem is like a rule that says this will happen, no matter what the individual candies weigh. As long as you have a lot of candies and you're picking a large enough group at a time, the average total weight of the candies you pick will be close to the average weight of an individual candy, no matter what that weight is.\n",
            "\n",
            "So, even if you have candies that weigh 1 gram or 100 grams, if you pick enough of them at a time, the average total weight will still be close to the average weight of an individual candy. That's the central limit theorem!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."
      ],
      "metadata": {
        "id": "sRyF9BdXB1rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge üòÄ\n",
        "Play around with this, and other, LLMs. keep a record of your findings:\n",
        "1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n",
        "2. Experiment with the parameters, one at a time, to assess their impact on the output.\n",
        "3. Attempt to load different models. **Remember**: you can only use models under 10GB for free. This means most 7B or 8B will work, but when you move closer to 11B or 13B models, they are unlikely to function on the free tier of EndPoints. Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. When you find a model you like, the repo id is at the top of the model card. Use this repo id to load the endpoint.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hm_UJRaelxR1L4WRBfPJZYQyy7OS4Bj4)\n"
      ],
      "metadata": {
        "id": "txlIhyawB43U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZOsorPVf656"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}