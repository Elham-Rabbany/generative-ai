{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative_ai/blob/main/0_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "zfuohDidliSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Settings"
      ],
      "metadata": {
        "id": "LfDWxVPylmyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "KiUrIGPdl7Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "!pip install -qqq -U faiss-cpu\n",
        "#!pip install -qqq -U langchain\n",
        "!pip install -qqq -U langchain-community\n",
        "!pip install -qqq -U langchain-huggingface\n",
        "!pip install -qqq -U pypdf\n",
        "!pip install -qqq -U streamlit"
      ],
      "metadata": {
        "id": "EPC2bgSPlyCp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "CAj83CxIl_or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "Yh_NskAsmA-T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab: token"
      ],
      "metadata": {
        "id": "2zklDMk5l2KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "vEViDb6ql5zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local: token"
      ],
      "metadata": {
        "id": "tIaJTonJmUQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
      ],
      "metadata": {
        "id": "OBvVTgaAmV3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "ZXpffJlkmcgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model = 'mistralai/Mistral-7B-Instruct-v0.3' # 'microsoft/Phi-3.5-mini-instruct'\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)"
      ],
      "metadata": {
        "id": "uWefNZbqmdsh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/14PqeK1VNokE5PZ-b-hVLqV-h4xnSR7B4/view?usp=sharing Alice in Wonderland\n",
        "\n",
        "https://drive.google.com/file/d/1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb/view?usp=sharing Statistics in Python\n",
        "\n",
        "https://drive.google.com/file/d/1i5ZhAxtIown7RzH1Sh13y2eqcXKvsMyq/view?usp=sharing Statistics in R\n",
        "\n",
        "https://drive.google.com/file/d/1TKm46XvBhaUSXTgGu5FJdHeSMG5njVUE/view?usp=sharing Index faiss\n",
        "\n",
        "https://drive.google.com/file/d/1VPzK6jhm7059HZ67CRSZwAGHvTuI_nn3/view?usp=sharing Index pkl"
      ],
      "metadata": {
        "id": "LttoXR4knZ4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding"
      ],
      "metadata": {
        "id": "wGg-llCPoe9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model\n",
        "    , cache_folder=embeddings_folder)"
      ],
      "metadata": {
        "id": "OndazxOCxVn9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: Create vector from loaded document"
      ],
      "metadata": {
        "id": "rumFfy1FxBkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1mJIwux2e0XvZUDBoEj9rb3OlHlYsTxXb' # Google drive file's ID\n",
        "file = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
        "\n",
        "loader = PyPDFLoader(file)\n",
        "docs = []\n",
        "async for page in loader.alazy_load():\n",
        "    docs.append(page)\n",
        "\n",
        "# # Alternatively load from text file\n",
        "# docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# # Review the loaded document\n",
        "# print(f\"{pages[0].metadata}\\n\")\n",
        "# print(pages[0].page_content)"
      ],
      "metadata": {
        "id": "1h6_PpaHw8aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "oYSGzYvRyGNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: Load vector from saved index"
      ],
      "metadata": {
        "id": "WcXRITtPxFpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "iiHxzE_AohNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever"
      ],
      "metadata": {
        "id": "nSS-OU-IycZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "3Q__zz6VybMq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat setup"
      ],
      "metadata": {
        "id": "aZw8ojnZomfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short, succinct and informative, so that the couterpart can learn from you.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "# chat_history = []\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "doc_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, prompt\n",
        ")\n",
        "\n",
        "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# rag_bot = create_retrieval_chain(\n",
        "#     doc_retriever, doc_chain\n",
        "# )"
      ],
      "metadata": {
        "id": "y9Tbv_08on9W"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatbot"
      ],
      "metadata": {
        "id": "wVDclu2qqZh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_retrieval_chain(\n",
        "    doc_retriever, doc_chain\n",
        ")\n",
        "history = []\n",
        "# Start the conversation loop\n",
        "while True:\n",
        "  user_input = input(\"\\nYou: \")\n",
        "\n",
        "  # Check for exit condition\n",
        "  if user_input.lower() == 'end':\n",
        "      print(\"Ending the conversation. Goodbye!\")\n",
        "      break\n",
        "\n",
        "  # Get the response from the conversation chain\n",
        "  response = chain.invoke({\"input\":user_input, \"chat_history\": history, \"context\": retriever})\n",
        "  history.extend([{\"role\": \"human\", \"content\": response[\"input\"]},{\"role\": \"assistant\", \"content\":response[\"answer\"]}])\n",
        "  # Print the chatbot's response\n",
        "  print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG89RzK5qcBS",
        "outputId": "1cf6882d-7796-4b6a-f1ff-ac66ce430ea6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You: What is the topic you can tell me about?\n",
            "\n",
            "Response: The topic is about Classification in Statistical Learning, which is the process of predicting a qualitative response, also known as a categorical variable. This chapter discusses various approaches for classification, such as logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes, and K-nearest neighbors. Additionally, it covers survival analysis and censored data, which are important topics in various applications, especially in medicine.\n",
            "\n",
            "You: In Python or in R?\n",
            "\n",
            "AI:\n",
            "Response: This topic, An Introduction to Statistical Learning, With Applications in Python (ISLP), is specifically about implementing the statistical learning methods in Python. However, the original version, An Introduction to Statistical Learning, With Applications in R (ISLR), is about implementing them in R.\n",
            "\n",
            "You: end\n",
            "Ending the conversation. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streamlit"
      ],
      "metadata": {
        "id": "4cED7Wah0l8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "app.py"
      ],
      "metadata": {
        "id": "hRAfwcEV1IPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFsof3G1NHL",
        "outputId": "b1f38341-704f-4695-da4e-bb7db31cb9c1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VVq050eE02v9",
        "outputId": "212957fe-6528-4caf-80cf-db8abb210f20"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'st' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-66e696a53d02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# bot with memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdoc_retriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_history_aware_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoc_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tunnel_prep():\n",
        "    for f in ('cloudflared-linux-amd64', 'logs.txt', 'nohup.out'):\n",
        "        try:\n",
        "            os.remove(f'/content/{f}')\n",
        "            print(f\"Deleted {f}\")\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "    !nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "    url = \"\"\n",
        "    while not url:\n",
        "        time.sleep(1)\n",
        "        result = !grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1\n",
        "        if result:\n",
        "            url = result[0]\n",
        "    return display(HTML(f'Your tunnel URL <a href=\"{url}\" target=\"_blank\">{url}</a>'))"
      ],
      "metadata": {
        "id": "tSFfQQfA0ndC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "0_chatbot.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}