{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gastan81/generative_ai/blob/main/0_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4lmzdFaqmgr"
      },
      "source": [
        "# Streamlit\n",
        "Streamlit lets you transform your data scripts into interactive dashboards and prototypes in minutes, without needing front-end coding knowledge. This means you can easily share insights with colleagues, showcase your data science work, or even build simple machine learning tools, all within the familiar Python environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2MN3UoMhEdj"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Streamlit demo 🚀\n",
        "We first need to install [streamlit](https://streamlit.io/) - as always, locally this is a one time thing, whereas on colab we need to do it each session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD62DdY_FSib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b46fbae-420a-41f4-ad97-cd345ecfc7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdx-68fIFVY3"
      },
      "source": [
        "To run Streamlit on Colab, we'll have to set up a tunnel. If you're working locally, you can skip this step.     \n",
        "Modified from [Source](https://colab.research.google.com/gist/thapecroth/67a69d840010ffcfe7523655808c5b92/streamlit-on-colab.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code necessary for Colab only\n",
        "\n",
        "import os\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "def tunnel_prep():\n",
        "    for f in ('cloudflared-linux-amd64', 'logs.txt', 'nohup.out'):\n",
        "        try:\n",
        "            os.remove(f'/content/{f}')\n",
        "            print(f\"Deleted {f}\")\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "    !nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "    url = \"\"\n",
        "    while not url:\n",
        "        time.sleep(1)\n",
        "        result = !grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1\n",
        "        if result:\n",
        "            url = result[0]\n",
        "    return display(HTML(f'Your tunnel URL <a href=\"{url}\" target=\"_blank\">{url}</a>'))"
      ],
      "metadata": {
        "id": "UyMwzTs8mDQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6IbJEYVtNFI"
      },
      "source": [
        "Here's an example of what you can produce with streamlit. It's so easy, just a few lines of python depending on what you want, and so many options!\n",
        "\n",
        "- Locally you would write this script in a .py file and not a notebook (.ipynb).\n",
        "\n",
        "- On colab, we can create a .py file by using the magic command `%%writefile` at the top of the cell. This command writes the cell content to a file, naming it 'app.py', or whatever else you choose, in this instance. Once saved, you can see 'app.py' in Colab's storage by clicking on the left-hand side folder icon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOXdinLGgriU",
        "outputId": "f19e5efc-9224-4c0d-a28a-95e9342ebe51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Title\n",
        "st.title(\"Streamlit Demo\")\n",
        "\n",
        "# Markdown\n",
        "st.markdown(\"\"\"\n",
        "This is a demo app showcasing a few of Streamlit's features.\n",
        "\n",
        "Streamlit is a powerful Python library for creating web apps. It is easy to use and has a wide range of features, including:\n",
        "\n",
        "* **Interactive widgets:** Streamlit makes it easy to create interactive widgets, such as sliders, dropdown menus, and radio buttons.\n",
        "* **Charts and graphs:** Streamlit can generate a variety of charts and graphs, including line charts, bar charts, and pie charts.\n",
        "* **Data display:** Streamlit can display data in a variety of ways, including tables, lists, and maps.\n",
        "* **Deployment:** Streamlit apps can be deployed to Heroku with a single command.\n",
        "\"\"\")\n",
        "\n",
        "# Slider\n",
        "slider_value = st.slider(\"Select a number:\", 0, 100)\n",
        "st.write(f\"You selected: {slider_value}\")\n",
        "\n",
        "# Dropdown menu\n",
        "dropdown_value = st.selectbox(\"Choose a color:\", [\"red\", \"green\", \"blue\"])\n",
        "st.write(f\"You chose: {dropdown_value}\")\n",
        "\n",
        "# Radio buttons\n",
        "radio_button_value = st.radio(\"Select a language:\", [\"English\", \"Spanish\", \"French\"])\n",
        "st.write(f\"You selected: {radio_button_value}\")\n",
        "\n",
        "# Text area\n",
        "text = st.text_area(\"Enter some text:\")\n",
        "if text:\n",
        "    st.write(f\"You entered: {text}\")\n",
        "\n",
        "# Button\n",
        "if st.button(\"Click me!\"):\n",
        "    st.write(\"You clicked the button!\")\n",
        "\n",
        "# Chart\n",
        "data = {\"x\": [1, 2, 3, 4, 5], \"y\": [6, 7, 2, 4, 5]}\n",
        "st.line_chart(data, x=\"x\")\n",
        "\n",
        "# Map\n",
        "map_data = [\n",
        "    {\"name\": \"New York\", \"lat\": 40.7128, \"lon\": -74.0060},\n",
        "    {\"name\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437},\n",
        "    {\"name\": \"Chicago\", \"lat\": 41.8783, \"lon\": -87.6233},\n",
        "]\n",
        "st.map(map_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37HjErWnt9-I"
      },
      "source": [
        "To run streamlit apps **locally**. Open the command line and navigate to the folder where you've stored the .py file. Then, use the command `streamlit run app.py`. If you used a different name for the .py file, change `app.py` for the name you used\n",
        "\n",
        "On **colab**, as we have to run streamlit through a tunnel. This is a little annoying for debugging, as every time you encounter a bug, you have to stop and reopen the tunnel. However, if you have a slower computer, or you simply wish to use Google's power so that your resources are free to do other things, it's very useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgeRO3S7JMym"
      },
      "source": [
        "### Local"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open a terminal and run this code. Make sure your current working directory is the same as contains your `app.py` file, and that you've activated an environment in which `streamlit` is installed.\n",
        "```\n",
        "streamlit run app.py\n",
        "```"
      ],
      "metadata": {
        "id": "UZ1CLB1S3UNL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRcmiyvoJPN2"
      },
      "source": [
        "### Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRcEI6cjgtrW",
        "outputId": "55f587d5-e665-4e87-ada4-d6955ff8e509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted cloudflared-linux-amd64\n"
          ]
        }
      ],
      "source": [
        "tunnel_prep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvDzNr5uHKPm"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUCdhQSihG5l"
      },
      "source": [
        "## 2.&nbsp; RAG chatbot in streamlit ⭐️\n",
        "We'll start by installing the same libraries and downloading the same files as in the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rOEYKWwhI-d",
        "outputId": "52f6906e-6336-4631-c017-432c003ccbdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 19.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 kB 21.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 38.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 3.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.5/27.5 MB 33.3 MB/s eta 0:00:00\n",
            "Processing file 1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt index.faiss\n",
            "Processing file 1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ index.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt\n",
            "To: /content/faiss_index/index.faiss\n",
            "\r  0%|          | 0.00/421k [00:00<?, ?B/s]\r100%|██████████| 421k/421k [00:00<00:00, 117MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ\n",
            "To: /content/faiss_index/index.pkl\n",
            "\r  0%|          | 0.00/216k [00:00<?, ?B/s]\r100%|██████████| 216k/216k [00:00<00:00, 91.8MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -qqq -U langchain-huggingface\n",
        "pip install -qqq -U langchain\n",
        "pip install -qqq -U langchain-community\n",
        "pip install -qqq -U faiss-cpu\n",
        "\n",
        "# download saved vector database for Alice's Adventures in Wonderland\n",
        "gdown --folder 1A8A9lhcUXUKRrtCe7rckMlQtgmfLZRQH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7cP3jlfg5jZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata # we stored our access token as a colab secret\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogWDZ5fbv7BN"
      },
      "source": [
        "Now, let's proceed by creating the .py file for our rag chatbot.\n",
        "\n",
        "We sourced the foundational code for our Streamlit basic chatbot from the [Streamlit documentation](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).\n",
        "\n",
        "In addition, we implemented [cache_resource](https://docs.streamlit.io/library/api-reference/performance/st.cache_resource) for both memory and LLM. Given that Streamlit reruns the entire script with each message input, relying solely on memory would result in data overwriting and a loss of conversational continuity. The inclusion of cache resource prevents Streamlit from creating a new memory instance on each run. This was also added to the LLM, enhancing speed and preventing its reload in every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kauMaNJ1hXNP",
        "outputId": "1233f405-7a0c-4460-edd9-10c37608e930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=hf_model)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqnlHR1Uv-88"
      },
      "source": [
        "Now, let's see what we've made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7H1ZevBJZzn"
      },
      "source": [
        "### Local"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code in a terminal, following the caveats as laid out previously.\n",
        "```\n",
        "streamlit run rag_app.py\n",
        "```"
      ],
      "metadata": {
        "id": "CqO8zHaR750D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcY2sm1VJZjd"
      },
      "source": [
        "### Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfAXxyZeI8tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da428b9-843c-4852-d59d-e54c2c8655be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tunnel url https://performed-surf-jones-gdp.trycloudflare.com\n"
          ]
        }
      ],
      "source": [
        "tunnel_prep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "236fopFmI8tn"
      },
      "outputs": [],
      "source": [
        "!streamlit run rag_app.py &>/content/logs.txt &"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "0_chatbot.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}